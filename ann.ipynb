{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c9736e0-88e7-4d65-908e-5f89c5f07fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df57d38-67cf-447e-b996-c7f554e31c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df19dbb3-602f-4b5a-a69f-140e9387225d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>500</th>\n",
       "      <th>channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002432</td>\n",
       "      <td>-0.044127</td>\n",
       "      <td>-0.168850</td>\n",
       "      <td>-0.404639</td>\n",
       "      <td>-0.773205</td>\n",
       "      <td>-1.290878</td>\n",
       "      <td>-1.967325</td>\n",
       "      <td>-2.804175</td>\n",
       "      <td>-3.795185</td>\n",
       "      <td>...</td>\n",
       "      <td>1.060049e-03</td>\n",
       "      <td>1.055896e-03</td>\n",
       "      <td>1.051771e-03</td>\n",
       "      <td>1.047676e-03</td>\n",
       "      <td>1.043609e-03</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>GNa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002408</td>\n",
       "      <td>-0.043682</td>\n",
       "      <td>-0.167142</td>\n",
       "      <td>-0.400537</td>\n",
       "      <td>-0.765338</td>\n",
       "      <td>-1.277670</td>\n",
       "      <td>-1.947037</td>\n",
       "      <td>-2.774963</td>\n",
       "      <td>-3.755146</td>\n",
       "      <td>...</td>\n",
       "      <td>8.714939e-04</td>\n",
       "      <td>8.683297e-04</td>\n",
       "      <td>8.651874e-04</td>\n",
       "      <td>8.620671e-04</td>\n",
       "      <td>8.589685e-04</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>GNa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002385</td>\n",
       "      <td>-0.043237</td>\n",
       "      <td>-0.165435</td>\n",
       "      <td>-0.396436</td>\n",
       "      <td>-0.757471</td>\n",
       "      <td>-1.264464</td>\n",
       "      <td>-1.926759</td>\n",
       "      <td>-2.745770</td>\n",
       "      <td>-3.715146</td>\n",
       "      <td>...</td>\n",
       "      <td>7.552828e-04</td>\n",
       "      <td>7.527204e-04</td>\n",
       "      <td>7.501869e-04</td>\n",
       "      <td>7.476833e-04</td>\n",
       "      <td>7.452114e-04</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>GNa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002301</td>\n",
       "      <td>-0.042753</td>\n",
       "      <td>-0.163701</td>\n",
       "      <td>-0.392315</td>\n",
       "      <td>-0.749590</td>\n",
       "      <td>-1.251251</td>\n",
       "      <td>-1.906479</td>\n",
       "      <td>-2.716589</td>\n",
       "      <td>-3.675181</td>\n",
       "      <td>...</td>\n",
       "      <td>6.761228e-04</td>\n",
       "      <td>6.741544e-04</td>\n",
       "      <td>6.721797e-04</td>\n",
       "      <td>6.702074e-04</td>\n",
       "      <td>6.682260e-04</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>GNa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002338</td>\n",
       "      <td>-0.042347</td>\n",
       "      <td>-0.162020</td>\n",
       "      <td>-0.388233</td>\n",
       "      <td>-0.741742</td>\n",
       "      <td>-1.238065</td>\n",
       "      <td>-1.886229</td>\n",
       "      <td>-2.687443</td>\n",
       "      <td>-3.635262</td>\n",
       "      <td>...</td>\n",
       "      <td>6.132600e-04</td>\n",
       "      <td>6.116717e-04</td>\n",
       "      <td>6.102570e-04</td>\n",
       "      <td>6.090416e-04</td>\n",
       "      <td>6.080535e-04</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>GNa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.098678</td>\n",
       "      <td>2.286103</td>\n",
       "      <td>2.583387</td>\n",
       "      <td>2.690380</td>\n",
       "      <td>2.777422</td>\n",
       "      <td>2.865207</td>\n",
       "      <td>2.936723</td>\n",
       "      <td>2.969902</td>\n",
       "      <td>2.948981</td>\n",
       "      <td>...</td>\n",
       "      <td>1.021853e-06</td>\n",
       "      <td>1.155975e-06</td>\n",
       "      <td>1.242497e-06</td>\n",
       "      <td>1.333977e-06</td>\n",
       "      <td>1.424657e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>GpCa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.109934</td>\n",
       "      <td>2.307669</td>\n",
       "      <td>2.606190</td>\n",
       "      <td>2.713114</td>\n",
       "      <td>2.800222</td>\n",
       "      <td>2.888272</td>\n",
       "      <td>2.960047</td>\n",
       "      <td>2.993270</td>\n",
       "      <td>2.972047</td>\n",
       "      <td>...</td>\n",
       "      <td>1.347513e-06</td>\n",
       "      <td>1.331165e-06</td>\n",
       "      <td>1.320712e-06</td>\n",
       "      <td>1.316883e-06</td>\n",
       "      <td>1.320442e-06</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>GpCa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.121183</td>\n",
       "      <td>2.329188</td>\n",
       "      <td>2.628915</td>\n",
       "      <td>2.735754</td>\n",
       "      <td>2.822920</td>\n",
       "      <td>2.911228</td>\n",
       "      <td>2.983256</td>\n",
       "      <td>3.016522</td>\n",
       "      <td>2.994997</td>\n",
       "      <td>...</td>\n",
       "      <td>1.509715e-06</td>\n",
       "      <td>1.511124e-06</td>\n",
       "      <td>1.515017e-06</td>\n",
       "      <td>1.521571e-06</td>\n",
       "      <td>1.530969e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>GpCa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.132426</td>\n",
       "      <td>2.350658</td>\n",
       "      <td>2.651563</td>\n",
       "      <td>2.758301</td>\n",
       "      <td>2.845515</td>\n",
       "      <td>2.934075</td>\n",
       "      <td>3.006353</td>\n",
       "      <td>3.039659</td>\n",
       "      <td>3.017833</td>\n",
       "      <td>...</td>\n",
       "      <td>7.954999e-07</td>\n",
       "      <td>7.776499e-07</td>\n",
       "      <td>7.985281e-07</td>\n",
       "      <td>8.632145e-07</td>\n",
       "      <td>9.769840e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>GpCa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.143662</td>\n",
       "      <td>2.372080</td>\n",
       "      <td>2.674134</td>\n",
       "      <td>2.780757</td>\n",
       "      <td>2.868010</td>\n",
       "      <td>2.956815</td>\n",
       "      <td>3.029337</td>\n",
       "      <td>3.062680</td>\n",
       "      <td>3.040554</td>\n",
       "      <td>...</td>\n",
       "      <td>1.345049e-06</td>\n",
       "      <td>1.409720e-06</td>\n",
       "      <td>1.485290e-06</td>\n",
       "      <td>1.570697e-06</td>\n",
       "      <td>1.664722e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>GpCa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1980 rows × 502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6  \\\n",
       "0     0.0 -0.002432 -0.044127 -0.168850 -0.404639 -0.773205 -1.290878   \n",
       "1     0.0 -0.002408 -0.043682 -0.167142 -0.400537 -0.765338 -1.277670   \n",
       "2     0.0 -0.002385 -0.043237 -0.165435 -0.396436 -0.757471 -1.264464   \n",
       "3     0.0 -0.002301 -0.042753 -0.163701 -0.392315 -0.749590 -1.251251   \n",
       "4     0.0 -0.002338 -0.042347 -0.162020 -0.388233 -0.741742 -1.238065   \n",
       "...   ...       ...       ...       ...       ...       ...       ...   \n",
       "1975  0.0  1.098678  2.286103  2.583387  2.690380  2.777422  2.865207   \n",
       "1976  0.0  1.109934  2.307669  2.606190  2.713114  2.800222  2.888272   \n",
       "1977  0.0  1.121183  2.329188  2.628915  2.735754  2.822920  2.911228   \n",
       "1978  0.0  1.132426  2.350658  2.651563  2.758301  2.845515  2.934075   \n",
       "1979  0.0  1.143662  2.372080  2.674134  2.780757  2.868010  2.956815   \n",
       "\n",
       "             7         8         9  ...           492           493  \\\n",
       "0    -1.967325 -2.804175 -3.795185  ...  1.060049e-03  1.055896e-03   \n",
       "1    -1.947037 -2.774963 -3.755146  ...  8.714939e-04  8.683297e-04   \n",
       "2    -1.926759 -2.745770 -3.715146  ...  7.552828e-04  7.527204e-04   \n",
       "3    -1.906479 -2.716589 -3.675181  ...  6.761228e-04  6.741544e-04   \n",
       "4    -1.886229 -2.687443 -3.635262  ...  6.132600e-04  6.116717e-04   \n",
       "...        ...       ...       ...  ...           ...           ...   \n",
       "1975  2.936723  2.969902  2.948981  ...  1.021853e-06  1.155975e-06   \n",
       "1976  2.960047  2.993270  2.972047  ...  1.347513e-06  1.331165e-06   \n",
       "1977  2.983256  3.016522  2.994997  ...  1.509715e-06  1.511124e-06   \n",
       "1978  3.006353  3.039659  3.017833  ...  7.954999e-07  7.776499e-07   \n",
       "1979  3.029337  3.062680  3.040554  ...  1.345049e-06  1.409720e-06   \n",
       "\n",
       "               494           495           496       497       498       499  \\\n",
       "0     1.051771e-03  1.047676e-03  1.043609e-03  0.001040  0.001036  0.001032   \n",
       "1     8.651874e-04  8.620671e-04  8.589685e-04  0.000856  0.000853  0.000850   \n",
       "2     7.501869e-04  7.476833e-04  7.452114e-04  0.000743  0.000740  0.000738   \n",
       "3     6.721797e-04  6.702074e-04  6.682260e-04  0.000666  0.000664  0.000662   \n",
       "4     6.102570e-04  6.090416e-04  6.080535e-04  0.000607  0.000607  0.000607   \n",
       "...            ...           ...           ...       ...       ...       ...   \n",
       "1975  1.242497e-06  1.333977e-06  1.424657e-06  0.000002  0.000002  0.000002   \n",
       "1976  1.320712e-06  1.316883e-06  1.320442e-06  0.000001  0.000001  0.000001   \n",
       "1977  1.515017e-06  1.521571e-06  1.530969e-06  0.000002  0.000002  0.000002   \n",
       "1978  7.985281e-07  8.632145e-07  9.769840e-07  0.000001  0.000001  0.000001   \n",
       "1979  1.485290e-06  1.570697e-06  1.664722e-06  0.000002  0.000002  0.000002   \n",
       "\n",
       "           500  channel  \n",
       "0     0.001028      GNa  \n",
       "1     0.000847      GNa  \n",
       "2     0.000736      GNa  \n",
       "3     0.000660      GNa  \n",
       "4     0.000605      GNa  \n",
       "...        ...      ...  \n",
       "1975  0.000002     GpCa  \n",
       "1976  0.000001     GpCa  \n",
       "1977  0.000001     GpCa  \n",
       "1978  0.000001     GpCa  \n",
       "1979  0.000002     GpCa  \n",
       "\n",
       "[1980 rows x 502 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw=pd.read_csv('ap_dataset_labeled.csv')\n",
    "raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dbf524-4285-487e-a196-ef5e2bb23917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d01143cd-abed-4933-8a75-dd51e376ddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X= raw.drop('channel', axis=1).values.astype(np.float32)  # shape: (1980, 501)\n",
    "labels = raw['channel'].values.tolist()                    # shape: (1980,)\n",
    "unique_labels = sorted(set(labels))\n",
    "label_to_int = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "y = np.array([label_to_int[label] for label in labels], dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ead07101-484e-4af5-b542-b6b59e2ac837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27d101a7-5727-476c-b19f-28d3199de149",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds   = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds  = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4fb08919-8135-467e-8513-f2805954624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(501,)),              # Input layer (AP waveform)\n",
    "    tf.keras.layers.Dense(130, activation='relu'),    # 🔥 One hidden layer\n",
    "    tf.keras.layers.Dense(10, activation='softmax')   # Output layer (10 classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84e16e51-9147-40d8-b252-4e9effcc5491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9422 - loss: 0.2284 - val_accuracy: 0.9663 - val_loss: 0.1815\n",
      "Epoch 2/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9476 - loss: 0.1955 - val_accuracy: 0.9663 - val_loss: 0.1640\n",
      "Epoch 3/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9609 - loss: 0.1659 - val_accuracy: 0.9663 - val_loss: 0.1578\n",
      "Epoch 4/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9613 - loss: 0.1581 - val_accuracy: 0.9697 - val_loss: 0.1499\n",
      "Epoch 5/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9666 - loss: 0.1473 - val_accuracy: 0.9731 - val_loss: 0.1392\n",
      "Epoch 6/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9642 - loss: 0.1491 - val_accuracy: 0.9764 - val_loss: 0.1324\n",
      "Epoch 7/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9595 - loss: 0.1691 - val_accuracy: 0.9697 - val_loss: 0.1295\n",
      "Epoch 8/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9673 - loss: 0.1325 - val_accuracy: 0.9596 - val_loss: 0.1318\n",
      "Epoch 9/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9535 - loss: 0.1763 - val_accuracy: 0.9057 - val_loss: 0.4337\n",
      "Epoch 10/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9199 - loss: 0.3065 - val_accuracy: 0.9630 - val_loss: 0.1353\n",
      "Epoch 11/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9609 - loss: 0.1545 - val_accuracy: 0.9731 - val_loss: 0.1232\n",
      "Epoch 12/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9665 - loss: 0.1262 - val_accuracy: 0.9697 - val_loss: 0.1162\n",
      "Epoch 13/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9636 - loss: 0.1384 - val_accuracy: 0.9731 - val_loss: 0.1098\n",
      "Epoch 14/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9672 - loss: 0.1199 - val_accuracy: 0.9731 - val_loss: 0.1044\n",
      "Epoch 15/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9697 - loss: 0.1222 - val_accuracy: 0.9731 - val_loss: 0.1014\n",
      "Epoch 16/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9715 - loss: 0.1142 - val_accuracy: 0.9731 - val_loss: 0.0980\n",
      "Epoch 17/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9700 - loss: 0.1193 - val_accuracy: 0.9731 - val_loss: 0.0949\n",
      "Epoch 18/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9688 - loss: 0.1135 - val_accuracy: 0.9731 - val_loss: 0.0922\n",
      "Epoch 19/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9742 - loss: 0.0987 - val_accuracy: 0.9798 - val_loss: 0.0900\n",
      "Epoch 20/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9793 - loss: 0.0955 - val_accuracy: 0.9798 - val_loss: 0.0879\n",
      "Epoch 21/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9730 - loss: 0.0997 - val_accuracy: 0.9798 - val_loss: 0.0857\n",
      "Epoch 22/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9806 - loss: 0.0914 - val_accuracy: 0.9798 - val_loss: 0.0831\n",
      "Epoch 23/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9806 - loss: 0.0848 - val_accuracy: 0.9865 - val_loss: 0.0814\n",
      "Epoch 24/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9793 - loss: 0.0915 - val_accuracy: 0.9798 - val_loss: 0.0923\n",
      "Epoch 25/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9818 - loss: 0.0830 - val_accuracy: 0.9865 - val_loss: 0.0830\n",
      "Epoch 26/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9851 - loss: 0.0822 - val_accuracy: 0.9832 - val_loss: 0.0768\n",
      "Epoch 27/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9809 - loss: 0.0811 - val_accuracy: 0.9832 - val_loss: 0.0737\n",
      "Epoch 28/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9802 - loss: 0.0785 - val_accuracy: 0.9832 - val_loss: 0.0725\n",
      "Epoch 29/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9825 - loss: 0.0643 - val_accuracy: 0.9865 - val_loss: 0.0703\n",
      "Epoch 30/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9782 - loss: 0.0888 - val_accuracy: 0.9865 - val_loss: 0.0703\n",
      "Epoch 31/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9850 - loss: 0.0790 - val_accuracy: 0.9832 - val_loss: 0.0689\n",
      "Epoch 32/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9846 - loss: 0.0697 - val_accuracy: 0.9865 - val_loss: 0.0707\n",
      "Epoch 33/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9696 - loss: 0.0968 - val_accuracy: 0.9865 - val_loss: 0.0673\n",
      "Epoch 34/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9843 - loss: 0.0712 - val_accuracy: 0.9865 - val_loss: 0.0639\n",
      "Epoch 35/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9832 - loss: 0.0752 - val_accuracy: 0.9865 - val_loss: 0.0631\n",
      "Epoch 36/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9855 - loss: 0.0568 - val_accuracy: 0.9865 - val_loss: 0.0615\n",
      "Epoch 37/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9853 - loss: 0.0625 - val_accuracy: 0.9865 - val_loss: 0.0604\n",
      "Epoch 38/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9791 - loss: 0.0767 - val_accuracy: 0.9832 - val_loss: 0.0630\n",
      "Epoch 39/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9908 - loss: 0.0584 - val_accuracy: 0.9865 - val_loss: 0.0588\n",
      "Epoch 40/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9844 - loss: 0.0678 - val_accuracy: 0.9832 - val_loss: 0.0583\n",
      "Epoch 41/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9830 - loss: 0.0642 - val_accuracy: 0.9865 - val_loss: 0.0564\n",
      "Epoch 42/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9889 - loss: 0.0567 - val_accuracy: 0.9865 - val_loss: 0.0594\n",
      "Epoch 43/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9863 - loss: 0.0549 - val_accuracy: 0.9832 - val_loss: 0.0543\n",
      "Epoch 44/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9877 - loss: 0.0597 - val_accuracy: 0.9865 - val_loss: 0.0529\n",
      "Epoch 45/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9880 - loss: 0.0602 - val_accuracy: 0.9865 - val_loss: 0.0520\n",
      "Epoch 46/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9837 - loss: 0.0588 - val_accuracy: 0.9899 - val_loss: 0.0514\n",
      "Epoch 47/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9862 - loss: 0.0591 - val_accuracy: 0.9832 - val_loss: 0.0521\n",
      "Epoch 48/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9869 - loss: 0.0496 - val_accuracy: 0.9899 - val_loss: 0.0492\n",
      "Epoch 49/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9898 - loss: 0.0487 - val_accuracy: 0.9899 - val_loss: 0.0484\n",
      "Epoch 50/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9881 - loss: 0.0482 - val_accuracy: 0.9865 - val_loss: 0.0503\n",
      "Epoch 51/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9898 - loss: 0.0456 - val_accuracy: 0.9899 - val_loss: 0.0464\n",
      "Epoch 52/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9904 - loss: 0.0478 - val_accuracy: 0.9899 - val_loss: 0.0468\n",
      "Epoch 53/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9910 - loss: 0.0405 - val_accuracy: 0.9899 - val_loss: 0.0461\n",
      "Epoch 54/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9901 - loss: 0.0427 - val_accuracy: 0.9899 - val_loss: 0.0441\n",
      "Epoch 55/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9906 - loss: 0.0440 - val_accuracy: 0.9966 - val_loss: 0.0442\n",
      "Epoch 56/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9933 - loss: 0.0394 - val_accuracy: 0.9966 - val_loss: 0.0446\n",
      "Epoch 57/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9886 - loss: 0.0501 - val_accuracy: 0.9899 - val_loss: 0.0426\n",
      "Epoch 58/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9909 - loss: 0.0420 - val_accuracy: 0.9966 - val_loss: 0.0415\n",
      "Epoch 59/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9931 - loss: 0.0400 - val_accuracy: 0.9899 - val_loss: 0.0419\n",
      "Epoch 60/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9929 - loss: 0.0433 - val_accuracy: 0.9933 - val_loss: 0.0421\n",
      "Epoch 61/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9843 - loss: 0.1393 - val_accuracy: 0.9966 - val_loss: 0.0489\n",
      "Epoch 62/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9926 - loss: 0.0400 - val_accuracy: 0.9899 - val_loss: 0.0421\n",
      "Epoch 63/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9854 - loss: 0.0470 - val_accuracy: 0.9933 - val_loss: 0.0399\n",
      "Epoch 64/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9899 - loss: 0.0470 - val_accuracy: 0.9899 - val_loss: 0.0404\n",
      "Epoch 65/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9903 - loss: 0.0420 - val_accuracy: 0.9899 - val_loss: 0.0392\n",
      "Epoch 66/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9903 - loss: 0.0419 - val_accuracy: 0.9899 - val_loss: 0.0395\n",
      "Epoch 67/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9912 - loss: 0.0409 - val_accuracy: 0.9899 - val_loss: 0.0399\n",
      "Epoch 68/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9880 - loss: 0.0435 - val_accuracy: 0.9966 - val_loss: 0.0401\n",
      "Epoch 69/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9919 - loss: 0.0367 - val_accuracy: 0.9899 - val_loss: 0.0367\n",
      "Epoch 70/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9946 - loss: 0.0346 - val_accuracy: 0.9966 - val_loss: 0.0361\n",
      "Epoch 71/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9911 - loss: 0.0414 - val_accuracy: 0.9899 - val_loss: 0.0358\n",
      "Epoch 72/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9881 - loss: 0.0424 - val_accuracy: 0.9933 - val_loss: 0.0353\n",
      "Epoch 73/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9915 - loss: 0.0368 - val_accuracy: 0.9899 - val_loss: 0.0354\n",
      "Epoch 74/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9937 - loss: 0.0357 - val_accuracy: 0.9899 - val_loss: 0.0351\n",
      "Epoch 75/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9941 - loss: 0.0322 - val_accuracy: 0.9899 - val_loss: 0.0343\n",
      "Epoch 76/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9938 - loss: 0.0346 - val_accuracy: 0.9899 - val_loss: 0.0351\n",
      "Epoch 77/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9954 - loss: 0.0324 - val_accuracy: 0.9966 - val_loss: 0.0334\n",
      "Epoch 78/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9929 - loss: 0.0319 - val_accuracy: 0.9899 - val_loss: 0.0348\n",
      "Epoch 79/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9938 - loss: 0.0320 - val_accuracy: 0.9966 - val_loss: 0.0316\n",
      "Epoch 80/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9923 - loss: 0.0366 - val_accuracy: 0.9966 - val_loss: 0.0314\n",
      "Epoch 81/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9935 - loss: 0.0327 - val_accuracy: 0.9899 - val_loss: 0.0321\n",
      "Epoch 82/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9958 - loss: 0.0276 - val_accuracy: 0.9966 - val_loss: 0.0311\n",
      "Epoch 83/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9935 - loss: 0.0315 - val_accuracy: 0.9966 - val_loss: 0.0310\n",
      "Epoch 84/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9932 - loss: 0.0305 - val_accuracy: 0.9899 - val_loss: 0.0316\n",
      "Epoch 85/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9954 - loss: 0.0272 - val_accuracy: 0.9966 - val_loss: 0.0299\n",
      "Epoch 86/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9922 - loss: 0.0342 - val_accuracy: 0.9899 - val_loss: 0.0309\n",
      "Epoch 87/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9922 - loss: 0.0331 - val_accuracy: 0.9966 - val_loss: 0.0291\n",
      "Epoch 88/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9962 - loss: 0.0260 - val_accuracy: 0.9966 - val_loss: 0.0288\n",
      "Epoch 89/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0293 - val_accuracy: 0.9966 - val_loss: 0.0290\n",
      "Epoch 90/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0579 - val_accuracy: 0.9327 - val_loss: 0.4708\n",
      "Epoch 91/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9302 - loss: 0.6065 - val_accuracy: 0.9091 - val_loss: 0.3016\n",
      "Epoch 92/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9445 - loss: 0.2166 - val_accuracy: 0.9832 - val_loss: 0.0452\n",
      "Epoch 93/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9835 - loss: 0.0461 - val_accuracy: 0.9899 - val_loss: 0.0367\n",
      "Epoch 94/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9882 - loss: 0.0388 - val_accuracy: 0.9933 - val_loss: 0.0352\n",
      "Epoch 95/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9919 - loss: 0.0341 - val_accuracy: 0.9933 - val_loss: 0.0334\n",
      "Epoch 96/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9875 - loss: 0.0357 - val_accuracy: 0.9966 - val_loss: 0.0323\n",
      "Epoch 97/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9910 - loss: 0.0329 - val_accuracy: 0.9966 - val_loss: 0.0310\n",
      "Epoch 98/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9859 - loss: 0.0384 - val_accuracy: 0.9933 - val_loss: 0.0310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e5862415a0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',   # since y = integer labels\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4fe5a195-1f69-4eb5-ae92-f275ab3366b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9727 - loss: 1.3354\n",
      "\n",
      " Final Test Accuracy: 0.9832\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_ds)\n",
    "print(f\"\\n Final Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11dbe6d-10f7-4d14-bac2-913643ac0027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
